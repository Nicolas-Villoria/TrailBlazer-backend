"""
Overpass API service - Fast trail data download
Much faster than page-by-page GPX downloads from OSM
"""
import requests
import json
import pickle
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional
from datetime import datetime, timedelta

from models import PointModel, BoxModel
from core.utils import get_logger
from core.config import STATIC_DIR

logger = get_logger("overpass_service")


class OverpassService:
    """
    Fast trail data download using Overpass API
    Downloads all trail data in a single request instead of page-by-page
    
    10-50x faster than OSM trackpoints API!
    """
    
    # Multiple Overpass API mirrors for reliability
    OVERPASS_URLS = [
        "https://overpass-api.de/api/interpreter",
        "https://overpass.kumi.systems/api/interpreter",
        "https://overpass.openstreetmap.ru/api/interpreter",
    ]
    CACHE_EXPIRY_DAYS = 30  # Cache data for 30 days
    MAX_AREA_SIZE = 0.5  # Max bounding box size in degrees (~50km)
    
    def __init__(self):
        self.cache_dir = Path(STATIC_DIR) / "overpass_cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def _get_cache_path(self, box: BoxModel) -> Path:
        """Get cache file path for bounding box"""
        box_hash = f"{box.bottom_left.lat}_{box.bottom_left.lon}_{box.top_right.lat}_{box.top_right.lon}"
        return self.cache_dir / f"{box_hash}.pkl"
    
    def _is_cache_valid(self, cache_path: Path) -> bool:
        """Check if cached data is still valid"""
        if not cache_path.exists():
            return False
        
        # Check if cache is older than expiry
        cache_time = datetime.fromtimestamp(cache_path.stat().st_mtime)
        age = datetime.now() - cache_time
        
        if age > timedelta(days=self.CACHE_EXPIRY_DAYS):
            logger.info(f"Cache expired (age: {age.days} days)")
            return False
        
        logger.info(f" Using cached data (age: {age.days} days)")
        return True
    
    def download_trails(self, box: BoxModel) -> List[Tuple[PointModel, PointModel]]:
        """
        Download hiking/walking trails from Overpass API
        
        Much faster than OSM trackpoints API:
        - Single request vs hundreds of pages
        - JSON response vs GPX parsing
        - Binary cache vs text files
        
        Args:
            box: Bounding box
            
        Returns:
            List of trail segments as (start_point, end_point) tuples
        """
        cache_path = self._get_cache_path(box)
        
        # Check cache first
        if self._is_cache_valid(cache_path):
            logger.info(f"üìÇ Loading trails from cache: {cache_path}")
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        
        # Validate bounding box size
        box_width = abs(box.top_right.lon - box.bottom_left.lon)
        box_height = abs(box.top_right.lat - box.bottom_left.lat)
        
        if box_width > self.MAX_AREA_SIZE or box_height > self.MAX_AREA_SIZE:
            logger.warning(f"‚ö†Ô∏è  Large bounding box ({box_width:.3f}¬∞ √ó {box_height:.3f}¬∞) - may timeout")
            logger.warning(f"   Consider using a smaller area (< {self.MAX_AREA_SIZE}¬∞ in each dimension)")
        
        # Build Overpass query for hiking trails
        query = self._build_query(box)
        
        logger.info(f"üì• Downloading trails from Overpass API...")
        logger.info(f"   Bounding box: {box.bottom_left.lat},{box.bottom_left.lon} to {box.top_right.lat},{box.top_right.lon}")
        
        # Try multiple Overpass mirrors for reliability
        last_error = None
        for i, url in enumerate(self.OVERPASS_URLS):
            try:
                logger.info(f"   Trying mirror {i+1}/{len(self.OVERPASS_URLS)}: {url}")
                
                # Make single API request (vs hundreds of OSM requests!)
                response = requests.post(
                    url,
                    data={"data": query},
                    timeout=120  # Longer timeout for large areas
                )
                response.raise_for_status()
                
                data = response.json()
                logger.info(f"‚úÖ Downloaded {len(data.get('elements', []))} OSM elements")
                
                # Convert to trail segments
                segments = self._extract_segments(data)
                logger.info(f"‚úÖ Extracted {len(segments)} trail segments")
                
                # Cache the results
                with open(cache_path, 'wb') as f:
                    pickle.dump(segments, f)
                logger.info(f"üíæ Cached trails to: {cache_path}")
                
                return segments
                
            except requests.Timeout as e:
                last_error = e
                logger.warning(f"‚è±Ô∏è  Timeout from {url}")
                if i < len(self.OVERPASS_URLS) - 1:
                    logger.info(f"   Trying next mirror...")
                continue
                
            except requests.RequestException as e:
                last_error = e
                logger.warning(f"‚ö†Ô∏è  Error from {url}: {e}")
                if i < len(self.OVERPASS_URLS) - 1:
                    logger.info(f"   Trying next mirror...")
                continue
        
        # All mirrors failed
        logger.error(f"‚ùå Error downloading from Overpass API: {last_error}")
        raise last_error
    
    def _build_query(self, box: BoxModel) -> str:
        """
        Build Overpass QL query for hiking trails
        
        Queries for:
        - highway=path (hiking paths)
        - highway=footway (walking paths)
        - highway=track (rural tracks)
        - route=hiking (marked hiking routes)
        """
        bbox_str = f"{box.bottom_left.lat},{box.bottom_left.lon},{box.top_right.lat},{box.top_right.lon}"
        
        # Optimized query with timeout and output limit
        query = f"""
        [out:json][timeout:90][maxsize:536870912];
        (
          way["highway"="path"]({bbox_str});
          way["highway"="footway"]({bbox_str});
          way["highway"="track"]["tracktype"~"grade[1-3]"]({bbox_str});
        );
        out geom;
        """
        
        return query
    
    def _extract_segments(self, data: Dict[str, Any]) -> List[Tuple[PointModel, PointModel]]:
        """
        Extract trail segments from Overpass API response
        
        Converts OSM ways to point-to-point segments
        """
        segments = []
        
        for element in data.get('elements', []):
            if element['type'] == 'way' and 'geometry' in element:
                # Get nodes from geometry
                nodes = element['geometry']
                
                # Create segments between consecutive nodes
                for i in range(len(nodes) - 1):
                    start = PointModel(
                        lat=nodes[i]['lat'],
                        lon=nodes[i]['lon']
                    )
                    end = PointModel(
                        lat=nodes[i + 1]['lat'],
                        lon=nodes[i + 1]['lon']
                    )
                    segments.append((start, end))
        
        return segments
    
    def clear_cache(self, box: Optional[BoxModel] = None):
        """Clear cached data for a specific box or all caches"""
        if box:
            cache_path = self._get_cache_path(box)
            if cache_path.exists():
                cache_path.unlink()
                logger.info(f"üóëÔ∏è  Cleared cache: {cache_path}")
        else:
            # Clear all caches
            for cache_file in self.cache_dir.glob("*.pkl"):
                cache_file.unlink()
            logger.info(f"üóëÔ∏è  Cleared all caches in {self.cache_dir}")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """Get information about cached data"""
        cache_files = list(self.cache_dir.glob("*.pkl"))
        
        total_size = sum(f.stat().st_size for f in cache_files)
        
        return {
            "cache_dir": str(self.cache_dir),
            "num_cached_areas": len(cache_files),
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "cache_expiry_days": self.CACHE_EXPIRY_DAYS
        }
